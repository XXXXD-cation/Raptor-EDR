

# **一份基于Golang的现代化EDR平台架构蓝图**

## **第一部分：基础架构战略**

本部分旨在确立指导整个项目的高阶原则，在深入探讨具体组件之前，解决关于结构与组织的核心哲学问题。

### **1.1. 架构北极星：一种务实的、符合Go语言习惯的方法**

在为一个复杂的系统（如EDR）选择架构时，一个关键的决策点在于如何在经典的软件工程模式与特定语言的哲学之间取得平衡。对于Go语言而言，其核心理念是简洁、明确和高效。盲目地将源自Java或C\#生态的、高度抽象的架构模式（如经典的“整洁架构”）生搬硬套到Go项目中，往往会引入不必要的复杂性，与语言本身的设计初衷背道而驰。

因此，本方案不推荐教条式地应用分层过多的整洁架构，而是提出一种混合模型。该模型以Go社区广泛接受的“标准项目布局”（Standard Go Project Layout）为基础，并融入“六边形架构”（Hexagonal Architecture，又称“端口与适配器模式”）的核心思想——依赖倒置原则。

这种架构决策的背后逻辑是多层次的。首先，EDR系统本质上是复杂的，包含多个清晰的业务领域：终端数据采集、数据流处理、规则分析、威胁情报整合、响应处置等。若缺乏明确的边界划分，项目将迅速演变成一个难以维护的“巨石”应用。经典的整潔架構雖然提供了優秀的關注點分離，但其多層次的接口和抽象在Go的語境下顯得過於繁瑣。

其次，Go的标准项目布局提供了一个简洁且被社区广泛理解的结构。其核心特征是/cmd目录用于存放程序入口，而/internal目录则用于存放项目私有代码。Go编译器会强制执行/internal目录的私有性，即外部项目无法导入其中的包，这为实现内部模块化提供了天然的屏障。然而，该布局本身并未规定/internal内部的依赖关系应如何管理，这对于大型项目至关重要。

最后，六边形架构的核心思想恰好弥补了这一空缺。它规定，应用的核心业务逻辑（Application Core）不应依赖于基础设施（Infrastructure），如数据库、Web框架或消息队列。相反，核心逻辑应定义其需要的“端口”（在Go中通常是接口），而基础设施代码则作为“适配器”来实现这些接口。

将这些理念结合，便形成了我们称之为“Go-六边形”的混合模型。具体实践如下：

1. **项目顶层结构**：遵循标准布局，使用/cmd存放所有可执行文件的main包，/internal存放所有非公开的业务逻辑和库代码，/pkg存放可供外部项目引用的共享库。  
2. **内部依赖管理**：在/internal目录内部，严格遵循依赖倒置原则。例如，一个名为rules-engine（规则引擎）的服务，其核心逻辑位于/internal/rules-engine/core。这个核心包会定义它所需要的接口，比如type EventRepository interface { FindEvents(...) }。  
3. **适配器实现**：与具体技术相关的实现代码，如与ClickHouse数据库交互的逻辑，将位于一个独立的“适配器”包中，例如/internal/platform/storage/clickhouse。这个包会实现rules-engine定义的EventRepository接口。  
4. **依赖注入**：在应用程序的启动入口（/cmd/.../main.go），将具体的适配器实例（如clickhouse.NewRepository()）注入到需要它的核心服务中。

通过这种方式，系统既保持了Go语言开发者熟悉的目录结构和包管理习惯，又获得了分层架构带来的高内聚、低耦合、易测试的优点，同时避免了过度抽象的陷阱。

### **1.2. 组织结构：采用单一代码仓库（Monorepo）的理由**

对于一个由多个紧密协作的组件（如跨平台Agent、多个后端微服务、共享库）构成的EDR平台，采用单一代码仓库（Monorepo）管理所有代码是提升项目内聚力和开发速度的战略性选择。

EDR系统的本质决定了其组件之间的高度关联性。终端Agent和后端服务器是同一枚硬币的两面。例如，Agent上报的遥测数据格式发生任何变更，都要求后端的数据接收服务、解析服务和分析服务进行同步修改。

在多代码仓库（Multi-repo）模型中，这种变更需要跨越多个仓库进行协调、打标和发布，引入了复杂的版本管理问题和潜在的部署不匹配风险。而在Monorepo中，此变更可以通过一次原子的提交（atomic commit）完成。所有组件的依赖关系都在代码层面清晰可见，CI/CD流水线可以一次性构建和测试所有受影响的组件，确保了整个系统的一致性。

具体优势体现在以下几个方面：

* **简化的依赖管理**：所有组件共享同一个Go模块，内部依赖直接通过路径导入，无需私有包管理或复杂的版本控制。共享库（如自定义的数据模型、安全原语）可以放在顶层的/pkg目录中，被任何服务或Agent直接使用。  
* **原子化的跨组件重构**：定义Agent与服务器通信协议的Protobuf文件可以存放在顶层的/proto目录。当协议变更时，可以在同一次提交中同时修改.proto文件、重新生成gRPC代码，并更新Agent和服务器的实现逻辑。这保证了两者之间的契约始终同步。  
* **统一的构建与测试流程**：CI/CD流水线可以配置为在每次提交时，构建和测试整个仓库。这确保了对共享代码的修改不会意外破坏任何依赖它的组件，极大地提高了代码质量和发布信心。开源项目如Velociraptor的实践也展示了将不同组件代码置于同一仓库的有效性。

综上所述，Monorepo为EDR这类复杂系统提供了一个强有力的组织模型，它将组件间的紧密关系转化为开发流程上的优势，从而加速迭代并降低协作成本。

## **第二部分：终端Agent：架构与实现**

终端Agent是EDR系统的“眼睛和手”，是整个平台数据来源的基石。其设计的核心挑战在于如何在实现深度、持续监控的同时，保持极低的系统资源占用、极高的稳定性，并具备一定的隐蔽性。

### **2.1. Agent核心职责与数据源**

Agent的核心使命是对终端设备进行持续的、低开销的监控和数据采集。它必须能够提供对终端活动的“实时可见性”，为后续的威胁检测和响应提供原始素材。根据业界最佳实践，Agent需要采集的关键数据源包括：

* **进程活动**：进程的创建、执行、终止，以及进程间的关系（父子进程）。  
* **文件系统活动**：文件的创建、修改、删除、重命名。  
* **网络连接**：出站和入站的网络连接、监听端口。  
* **用户活动**：用户登录、注销、权限提升。  
* **注册表/配置变更**（Windows）：对关键注册表键值的读写操作。  
* **模块加载**：进程加载动态链接库（DLL）或共享对象（SO）。

这些原始遥测数据是识别攻击指标（IOA, Indicators of Attack）和失陷指标（IOC, Indicators of Compromise）的基础。


### **2.2. 传感层核心：深入内核级事件采集**

传感层（Sensor）是Agent技术含量最高的部分，负责从操作系统内核获取高保真度的事件。为保证Agent核心逻辑的平台无关性，传感层需要设计一个抽象接口，由各平台的具体实现来满足。

#### **2.2.1. Linux传感：eBPF的纯Go优势**

在现代Linux系统上，eBPF（extended Berkeley Packet Filter）已成为进行内核观测和插桩的事实标准。它允许在内核中运行一个安全的沙箱化程序，以极高的性能捕获系统调用等底层事件。

对于本EDR项目，强烈推荐使用cilium/ebpf库来实现Linux传感层。这是一个纯Go语言实现的eBPF库，不依赖CGO。这一特性带来了巨大的工程优势。Go语言项目中的CGO常常是构建和交叉编译的痛点，它要求每个目标构建环境都必须安装匹配的C工具链和头文件。而cilium/ebpf库的纯Go特性使得交叉编译变得极其简单：开发者可以在macOS或Windows上，通过一条GOOS=linux GOARCH=amd64 go build命令，轻松构建出生产级的Linux Agent二进制文件。

这种构建上的简洁性极大地加速了开发-测试-部署的循环。虽然另一个流行的库libbpfgo（基于CGO对官方libbpf的封装）可能能更快地支持最新的内核特性，但对于EDR所需的核心功能（如通过kprobes/tracepoints挂钩execve, openat, connect等关键系统调用，并通过perf/ring buffer向用户空间高效传递数据），cilium/ebpf已经提供了成熟且稳定的支持 1。在商业级产品的开发中，可预测的构建流程和开发效率的重要性，往往超过对最前沿内核特性的追求。因此，选择

cilium/ebpf是一个更具战略眼光的决策。

#### **2.2.2. Windows传感：ETW与API封装**

对于Windows平台，ETW（Event Tracing for Windows）是首选的数据来源。ETW是Windows内置的高性能跟踪设施，能够以结构化事件的形式提供丰富的内核和应用层活动信息，包括进程创建、网络活动、镜像加载等。

要使用Go消费ETW事件，需要直接与Windows API交互。这可以通过Go标准库的syscall包、官方扩展库golang.org/x/sys/windows以及社区提供的更高级别的封装库来实现。例如，elastic/go-windows 和 iamacarpet/go-win64api 都提供了对复杂Windows API的CGO-free封装，可以简化开发。

对于ETW未能覆盖或覆盖不深的领域（如某些细粒度的文件操作），未来可以考虑采用更具侵入性的API挂钩（API Hooking）技术作为补充，但这将显著增加实现的复杂性和稳定性风险，应作为后续增强功能审慎评估。

#### **2.2.3. 跨平台系统信息采集**

除了内核级事件，Agent还需要采集高阶的系统上下文信息，用于丰富遥测数据和进行资产盘点。shirou/gopsutil库是此项任务的理想选择。它是一个跨平台的纯Go库，能够获取CPU/内存使用率、运行中进程列表、磁盘分区、登录用户等信息。其纯Go、无CGO的特性与我们为Linux传感层选择cilium/ebpf的理念完全一致，保证了整个Agent项目在构建上的统一和简洁。

### **2.3. Agent子模块：配置、通信与控制**

除了传感层，一个功能完备的Agent还需要以下关键模块：

* **配置模块 (/internal/agent/config)**: 采用spf13/viper库进行配置管理。viper支持从多种格式的文件（如YAML, TOML, JSON）中读取配置，并且允许通过环境变量进行覆盖。这对于在容器化环境中部署Agent至关重要（遵循十二要素应用原则）。此外，viper的WatchConfig功能可以实现配置文件的热加载，允许在不重启Agent的情况下更新其行为。  
* **通信模块 (/internal/agent/comms)**: Agent与服务器之间的所有通信都应通过基于gRPC的双向TLS（mTLS）进行。gRPC协议基于HTTP/2，性能高效，并通过Protobuf提供强类型的数据契约。其支持双向流式传输的特性，非常适合遥测数据的持续上报和服务器任务的实时下发。  
* **任务模块 (/internal/agent/tasking)**: Agent必须能够响应服务器下发的指令。这些指令可能包括：“执行一次文件系统扫描”、“隔离当前主机”、“获取指定文件”等。此模块负责解析任务并调度执行。可以借鉴Velociraptor的设计，它使用一种名为VQL的查询语言来定义和执行终端上的取证任务，提供了极大的灵活性。  
* **处置模块 (/internal/agent/remediation)**: 作为“响应”（Response）能力的核心，Agent必须能够执行具体的安全处置操作，如终止恶意进程、删除恶意文件、通过修改防火墙规则隔离主机等。这些操作通常需要高权限，并且实现时必须极其谨慎以防误操作。在Linux上，可以使用vishvananda/netlink库来与内核的Netlink套接字交互，从而编程方式地管理网络接口和防火墙规则，实现主机隔离。

## **第三部分：后端平台：可扩展的微服务架构**

后端平台是EDR的“大脑”，负责接收、存储、分析来自成千上万个终端的海量数据，并最终产生威胁告警。为应对这种规模和复杂性，设计一个分布式的、可水平扩展的微服务架构是必然选择。

### **3.1. 系统概览与服务拆分**

EDR平台天然适合云原生部署以获得弹性伸缩能力。其复杂的功能集合——数据接收、扩充、实时分析、批处理分析、调查取证、API服务等——非常适合通过微服务进行解耦，从而实现独立开发、独立部署和独立扩展。

基于数据处理流水线的思想，建议将后端平台拆分为以下微服务：

* **Ingest-Gateway (接收网关)**: 负责终结来自Agent的mTLS/gRPC连接。此服务逻辑应尽可能简单，仅做身份验证和数据格式初步校验，然后将遥测数据快速推送到内部消息总线。  
* **Enrichment-Service (扩充服务)**: 从消息总线消费原始遥测数据，并为其添加上下文信息。例如，根据IP地址关联地理位置信息，根据文件哈希查询威胁情报库，或关联资产清单中的主机信息。  
* **Real-time-Analytics-Service (实时分析服务)**: 这是一个“快车道”分析引擎。它消费扩充后的事件流，并运行一系列轻量级的检测规则（如Sigma、YARA）。一旦匹配，立即生成告警并推送到告警消息队列。  
* **Persistence-Service (持久化服务)**: 消费扩充后的遥测数据，并将其写入合适的长期存储数据库中。  
* **Batch-Analytics-Service (批处理分析服务)**: 这是一个“慢车道”分析引擎。它定期对历史数据进行复杂的分析，例如运行机器学习模型以发现异常行为序列，或检测“低慢”型攻击（low and slow attacks）。  
* **API-Gateway (API网关)**: 作为Web前端和所有第三方系统（如SIEM, SOAR）的统一入口。它对外提供RESTful或GraphQL API，并将请求路由到相应的内部服务。  
* **Identity-Service (身份服务)**: 负责管理平台用户的认证和授权。

下表清晰地定义了每个微服务的职责边界和技术选型，为开发团队提供了明确的指引。

**表1：后端平台微服务职责划分**

| 服务名称 | 核心职责 | 主要协议 | 关键库/依赖 | 数据存储交互 | 下游服务 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| Ingest-Gateway | 接收Agent遥测数据，认证Agent | gRPC (mTLS) | google.golang.org/grpc | 无 | 消息总线 (NATS) |
| Enrichment-Service | 为遥测数据添加上下文（威胁情报、资产信息） | NATS | 自定义威胁情报客户端 | 威胁情报库 (Redis/Postgres) | 消息总线 (NATS) |
| Real-time-Analytics-Service | 在事件流上运行实时检测规则（Sigma, YARA） | NATS | Go-YARA, Go-Sigma | 无 | 消息总线 (NATS) |
| Persistence-Service | 将遥测数据和告警写入长期存储 | NATS | ClickHouse/Postgres/Neo4j客户端 | 时序数据库, 关系数据库, 图数据库 | 无 |
| Batch-Analytics-Service | 在历史数据上运行复杂的行为模型和ML算法 | NATS, DB-Query | Gota, Gonum (或通过API调用Python服务) | 时序数据库, 图数据库 | 消息总线 (NATS) |
| API-Gateway | 为前端和第三方提供统一的API接口 | REST/GraphQL | Gin, GQLGen | 身份服务, 关系数据库, 图数据库 | 各内部服务 |
| Identity-Service | 管理用户认证与授权 (JWT) | REST/gRPC | golang-jwt/jwt | 关系数据库 (Postgres) | API-Gateway |

### **3.2. 数据接收与处理流水线**

为了处理EDR系统产生的“海量遥测数据”，必须设计一个高吞吐、高可用的数据流水线。该流水线的核心是一个持久化的消息总线，如Apache Kafka或NATS JetStream。消息总线将数据接收（Ingestion）与数据处理（Processing）解耦，带来了巨大的架构优势：

* **削峰填谷**：能够平滑处理突发的数据洪峰。  
* **弹性**：如果下游某个处理服务发生故障，数据会暂存在消息总线中，待服务恢复后可继续处理，避免数据丢失。  
* **可扩展性**：可以通过增加消费者实例的数量来水平扩展数据处理能力。

在Kafka和NATS之间，对于一个以Go为主要技术栈的项目，NATS JetStream是一个更具吸引力的选择。NATS本身就是用Go语言编写的，拥有非常出色的Go客户端库，并且其JetStream持久化层提供了EDR流水线所需的“至少一次”投递语义。更重要的是，相比于通常需要依赖Zookeeper（或新的KRaft模式）的Kafka，NATS的部署和运维复杂度要低得多。对于绝大多数EDR部署场景，NATS JetStream在提供足够高的吞吐量和持久性的同时，带来了更低的运维成本和更佳的开发者体验。

### **3.3. 多层检测引擎**

一个有效的EDR系统必须结合多种检测技术，因为没有任何一种技术可以应对所有类型的威胁。本架构通过专门的分析服务支持多层次的检测策略：

* **实时分析服务 (快车道)**: 该服务订阅扩充后的事件流，并应用一系列预编译的、计算开销较低的规则。这包括：  
  * **IOC匹配**：匹配已知的恶意文件哈希、IP地址、域名等。  
  * **Sigma规则**：Sigma是一种通用的、开放的日志事件检测规则格式，非常适合用于流式事件的实时检测。  
  * YARA规则：YARA用于在文件或进程内存中匹配恶意代码的模式。  
    当规则命中时，该服务会生成一个结构化的告警事件，并将其发布到一个专门的“告警”主题上，供后续的持久化和通知服务消费。  
* **批处理分析服务 (慢车道)**: 该服务不处理实时事件流，而是定期（例如，每小时）对长期存储中的历史数据进行查询和分析。这种方式适用于检测那些在短时间内难以发现、需要基于更长时间跨度和更广数据范围的攻击模式，例如：  
  * **行为序列分析 (IOA)**：检测符合特定攻击战术（如MITRE ATT\&CK框架中描述的技术）的事件序列。例如，“一个Office文档通过宏执行了PowerShell，该PowerShell下载了一个文件，然后创建了一个计划任务”。  
  * **机器学习/AI**：通过训练模型来识别与正常基线行为的偏差。例如，一个用户账号突然在非工作时间从一个不常见的地理位置登录，并访问了大量敏感文件。  
  * **威胁猎捕查询**：支持安全分析师运行复杂的、探索性的查询来主动寻找未知威胁。

### **3.4. 多样化持久层：为正确的工作选择正确的工具**

试图将EDR平台所有类型的数据都塞进同一种数据库（无论是关系型还是NoSQL）是一种反模式，这必然会导致性能瓶颈和应用逻辑的过度复杂化。一个高性能、可扩展的EDR后端必须采用多样化持久层（Polyglot Persistence）策略。

这种策略的依据在于EDR系统处理的数据具有截然不同的特征和访问模式：

1. **事件遥测数据**: 这是典型的时序数据，以极高的速率持续写入（append-only）。查询通常基于时间范围、主机ID等维度进行聚合和筛选。对于这种工作负载，列式存储数据库（Columnar Database）是最佳选择，如ClickHouse或TimescaleDB。它们提供了极高的数据压缩率和闪电般的查询速度。VMware Carbon Black EDR使用Apache Solr，也是一种基于搜索索引的有效方案，尤其适合文本密集型事件的查询。  
2. **攻击图谱与关联关系**: 安全分析师需要理解事件之间的因果链：“进程A”派生了“进程B”，“进程B”写入了“文件C”，并连接到“IP地址D”。这种“谁做了什么”的关系本质上是一个图（Graph）。如果用关系型数据库存储，查询这种关联需要进行多次、昂贵的JOIN或递归查询。而使用原生的图数据库（Graph Database），如Neo4j或ArangoDB，可以非常高效地进行图遍历，从而轻松实现“攻击链可视化”等高级功能。  
3. **平台元数据**: 用户账户、Agent注册信息、策略配置、资产清单等是高度结构化的关系型数据。这类数据要求强一致性、支持事务。传统的RDBMS，如PostgreSQL，是处理这类数据最可靠、最成熟的工具。

因此，Persistence-Service将扮演数据路由器的角色，根据数据类型将其分发到最合适的数据库中，从而为上层应用提供一个干净、统一的数据访问抽象。

### **3.5. 管理API与Web控制台后端**

API-Gateway服务是用户与EDR平台交互的窗口。它将使用一个高性能的Go Web框架来构建，例如Gin 或 Echo。Gin因其庞大的社区、丰富的中间件和出色的性能而成为一个稳健的选择。

该服务将向前端的单页应用（SPA）暴露一组RESTful或GraphQL API。前端控制台必须提供清晰、高效的用户界面，以支持安全分析师的核心工作流程，包括告警分类、事件调查、威胁猎捕、报告生成和策略管理。

## **第四部分：核心库与横切关注点**

为了确保整个项目代码的一致性、质量和可维护性，必须对所有组件使用的基础库和通用模式进行标准化。

### **4.1. 配置管理：spf13/viper**

spf13/viper将被指定为整个项目的标准配置管理库。它将被用于所有Agent和后端微服务。viper的强大之处在于其分层配置能力：

1. **设置默认值**：在代码中为每个配置项设置一个合理的默认值。  
2. **读取配置文件**：从一个或多个路径读取配置文件（如config.yaml）。  
3. **环境变量覆盖**：允许通过环境变量覆盖任何配置项。例如，EDR\_DATABASE\_HOST环境变量将自动覆盖配置文件中的database.host。这对于遵循十二要素应用原则和在Docker/Kubernetes环境中运行至关重要。  
4. **命令行标志覆盖**：允许通过命令行参数进行最高优先级的覆盖。  
5. **热加载**：支持在运行时监控配置文件的变化并自动重新加载，无需重启服务。

### **4.2. 高性能结构化日志：rs/zerolog**

日志是分布式系统的生命线。本项目将采用结构化日志（Structured Logging）作为标准，所有日志输出必须为JSON格式。rs/zerolog库因其极致的性能、零内存分配的特性以及流畅的链式API而被选为标准日志库。虽然Uber的zap库 也是一个优秀的备选，但zerolog的API通常被认为更符合人体工程学。

为了保证日志格式和上下文的统一，将在/pkg/logger中创建一个共享的日志初始化包。所有服务在启动时都调用此包来获取一个预配置的、包含服务名称等标准字段的logger实例。

### **4.3. Agent-服务器通信协议：gRPC**

Agent与服务器之间的所有通信都将通过gRPC进行。其接口定义（服务和消息）将使用Protocol Buffers语言编写，并统一存放在项目顶层的/proto目录中。这个目录将成为Agent和服务器之间数据契约的“单一事实来源”。

选择gRPC的理由包括：

* **性能**：基于HTTP/2，使用二进制序列化，比传统的JSON/REST开销更小、速度更快，这对于从大量Agent传输高频遥测数据至关重要。  
* **强类型**：Protobuf提供了严格的类型检查，可以在编译时发现数据格式不匹配的问题。  
* **流式传输**：原生支持客户端流、服务器流和双向流，非常适合EDR场景中的遥测持续上报和任务实时下发。  
* **生态系统**：Go语言对gRPC的支持非常成熟和完善。

**表2：推荐核心技术栈**

| 类别 | 推荐库/工具 | 理由 / 关键用例 |
| :---- | :---- | :---- |
| **项目结构** | Go标准布局 \+ 六边形架构原则 | 结合了Go语言的习惯用法和现代架构的可测试性、模块化优点。 |
| **代码仓库管理** | Monorepo | 简化依赖管理，实现原子化提交，统一CI/CD，特别适合组件紧密耦合的系统。 |
| **配置管理** | spf13/viper | 支持文件、环境变量、默认值和热加载的分层配置，是Go生态的事实标准。 |
| **结构化日志** | rs/zerolog | 极致性能，零内存分配，流畅的API，适合高吞吐量的日志场景。 |
| **Web框架 / API网关** | gin-gonic/gin | 性能高，中间件生态丰富，社区活跃，是构建REST/GraphQL API的稳健选择。 |
| **Agent-服务器通信** | gRPC | 高性能、强类型、支持流式传输的RPC框架，适用于大规模设备通信。 |
| **Linux内核传感** | cilium/ebpf | 纯Go实现的eBPF库，避免CGO依赖，极大地简化了交叉编译和部署。 |
| **Windows内核传感** | ETW \+ golang.org/x/sys/windows | 使用Windows原生的高性能事件跟踪框架，结合官方系统调用包。 |
| **跨平台系统信息** | shirou/gopsutil | 纯Go实现的跨平台系统信息库，无CGO依赖，与整体架构理念一致。 |
| **消息总线** | NATS JetStream | Go原生，高性能，运维简单，为EDR数据流水线提供了可靠的持久化消息队列。 |
| **时序数据存储** | ClickHouse / TimescaleDB | 专为时序数据优化的列式数据库，提供极高的压缩率和查询性能。 |
| **图数据存储** | Neo4j / ArangoDB | 原生图数据库，高效处理事件关联关系，支持攻击链分析和可视化。 |
| **关系型数据存储** | PostgreSQL | 成熟、可靠的关系型数据库，用于存储平台元数据、用户和策略信息。 |

## **第五部分：综合与战略建议**

本部分对前述架构设计进行总结，并为项目的实施和未来发展提供战略性路线图。

### **5.1. 架构决策总结**

本报告提出的EDR平台架构蓝图基于一系列关键决策，旨在构建一个现代化、可扩展且可维护的系统：

* **架构模型**：采用“Go-六边形”混合模型，在Go标准项目布局的基础上应用依赖倒置原则，实现了Go语言习惯与现代架构思想的平衡。  
* **组织结构**：采用Monorepo来管理所有代码，以应对组件间的高度耦合，简化开发和CI/CD流程。  
* **Agent设计**：传感层采用平台原生技术（Linux上的eBPF，Windows上的ETW）以获取最高保真度的数据，并优先选择纯Go实现（如cilium/ebpf）以简化构建和部署。  
* **后端架构**：基于微服务和消息总线构建可扩展的数据处理流水线，将不同职责清晰地分离到独立的服务中。  
* **持久化策略**：采用多样化持久层，为不同类型的数据（时序遥测、图关联、关系元数据）选择最合适的存储技术，以实现最优性能。

### **5.2. 建议的开发路线图（MVP优先）**

为了降低项目风险并尽快验证核心价值，建议采用分阶段的、MVP（最小可行产品）优先的开发策略：

* **第一阶段 (核心MVP)**:  
  * **目标**: 验证端到端的数据流水线。  
  * **范围**:  
    * **Agent**: 仅支持Linux平台，只采集进程创建事件。  
    * **后端**: 实现Ingest-Gateway、一个基础的Persistence-Service（仅使用时序数据库）和API-Gateway。  
    * **前端**: 一个能按主机查看原始事件流的简单页面。  
  * **产出**: 一个可以从Linux终端接收事件并展示在网页上的基本系统。  
* **第二阶段 (扩充与基础检测)**:  
  * **目标**: 实现基础的威胁检测能力。  
  * **范围**:  
    * **Agent**: 增加对文件和网络事件的采集。  
    * **后端**: 开发Enrichment-Service（集成基础威胁情报）和Real-time-Analytics-Service（支持Sigma规则）。  
    * **前端**: 开发告警展示和分类界面。  
  * **产出**: 系统能够根据简单的规则生成并展示告警。  
* **第三阶段 (全功能集)**:  
  * **目标**: 达到一个功能相对完备的EDR产品形态。  
  * **范围**:  
    * **Agent**: 增加对Windows平台的支持，并实现基础的响应处置动作。  
    * **后端**: 引入图数据库和Batch-Analytics-Service，开始研发行为分析模型。  
    * **前端**: 实现攻击链可视化、威胁猎捕查询界面。  
  * **产出**: 一个跨平台、具备高级分析和响应能力的EDR平台。  
* **第四阶段 (规模化与加固)**:  
  * **目标**: 提升产品在真实环境中的性能、稳定性和安全性。  
  * **范围**:  
    * 进行大规模性能和压力测试，优化瓶颈。  
    * 增强Agent的自我保护和抗干扰能力。  
    * 完善平台的运维、监控和告警体系。

### **5.3. 面向未来：通往XDR之路**

本架构在设计之初就考虑了未来的可扩展性，特别是向XDR（Extended Detection and Response，扩展检测与响应）的演进。XDR的核心思想是打破数据孤岛，将来自终端、网络、云、电子邮件等多个安全领域的遥测数据进行统一分析和关联。

本方案的微服务架构和消息总线为这种演进提供了天然的土壤。当需要集成新的数据源时（例如，AWS CloudTrail日志、Office 365审计日志或网络流量探针数据），只需开发一个新的“采集器”微服务。该服务负责从新的数据源获取数据，将其转换为平台的标准事件格式，然后发布到同一个NATS消息总线中。

一旦数据进入流水线，它就可以被现有的Enrichment-Service、Real-time-Analytics-Service和Batch-Analytics-Service进行处理和分析，并与来自终端的数据进行自动关联。这种“插件式”的扩展能力意味着，将EDR平台升级为XDR平台，无需对核心架构进行颠覆性改造，只需不断增加新的数据“适配器”即可。这保证了架构的长期生命力和投资回报率。